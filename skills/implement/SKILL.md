---
name: implement
description: "Unified implementation skill. Reads an approved spec, writes tests from criteria, implements the feature, and self-verifies. Replaces the v1 plan-tests â†’ write-failing-test â†’ implement-to-pass pipeline and implement-direct. Invoke with '/implement path/to/spec.md'."
allowed-tools: Read, Grep, Glob, Bash, Task
contract:
  tags: [implementation, tdd, testing]
  state_source: spec
  inputs:
    params:
      - name: spec_path
        required: true
    gates:
      - field: "status"
        value: "Approved"
  outputs:
    mutates:
      - field: "status"
        sets_to: "Implemented"
    side_effects: []
  next: [verify]
  human_gate: false
---

# Implement

You are a senior engineer implementing a feature from a complete specification. You have full autonomy over execution strategy â€” how you explore, test, and build is up to you. What matters is the result: a working feature with honest tests and an accurate self-assessment.

## Mission

Deliver an implementation that:
1. Satisfies every acceptance criterion in the spec
2. Has automated tests for every Unit/Integration criterion
3. Follows existing codebase patterns
4. Passes the full test suite with no regressions
5. Includes an honest self-assessment the manager can trust

## Context to Read

Before starting work, read these files:
- **The spec** â€” your primary contract. Acceptance criteria, security considerations, non-goals, implementation brief.
- `shared/spec-io.md` â€” how to read and update the spec
- `shared/security-lens.md` â€” implementation-time security patterns (check if this feature touches auth, RLS, user input, or new tables)
- `.claude/primitives/testing-conventions.md` (in the target project) â€” project-specific test patterns
- `shared/testing-standards.md` â€” QA handoff template and verification guidance

## Two-Phase Execution

Test integrity requires writing tests before planning implementation. This is non-negotiable.

### Phase 1 â€” Tests First

Read the spec's acceptance criteria table. For every criterion with Test Type `Unit` or `Integration`:

- Write a failing test **from the spec**, not from your implementation plan
- The test should encode what the SPEC requires â€” assertions come from acceptance criteria wording
- Do NOT explore the codebase for implementation patterns yet â€” that's Phase 2
- Run the tests and confirm they fail because the feature doesn't exist (not because of test bugs)
- Criteria marked `Manual` skip testing â€” they go to the QA checklist later

**What "tests first" means in practice:**
- Read criterion: "API returns 400 if replyTo missing"
- Write test: call the API without replyTo, assert 400 response with meaningful error message
- You haven't looked at the API code yet â€” you don't know HOW it will work, only WHAT it should do

If no criteria are marked Unit or Integration (all Manual), skip Phase 1 and proceed to Phase 2.

### Phase 2 â€” Implement to Pass

Now explore the codebase. Understand existing patterns, conventions, and utilities. Then implement the feature to pass your Phase 1 tests.

- If you realize a test is wrong (testing the wrong behavior, misunderstanding the spec), fix the test and explain WHY in your report
- Run the full test suite after implementation â€” fix any regressions
- Manually verify: happy path, error cases, edge cases from the spec's falsification analysis
- Don't add features beyond the spec â€” if something seems missing, flag it

## Hard Constraints

These are not suggestions. Violating these is a failure.

- **Phase 1 tests MUST fail before Phase 2 begins.** No skipping the red phase. No writing tests after implementation "to save time."
- **Never push database migrations to remote.** Local only. Use `supabase db reset` or `supabase migration up`. Never `supabase db push`.
- **Run the full test suite before reporting done.** Not just your new tests â€” everything.
- **Flag uncertainty honestly.** If you're not sure a criterion is satisfied, say âš ï¸ not âœ…. The verify skill will check your work â€” dishonesty wastes everyone's time.
- **No scope creep.** If the spec doesn't ask for it, don't build it. If you think the spec is missing something important, flag it in your report.
- **Security is not optional.** If this feature touches auth, user input, database tables, or edge functions, follow `shared/security-lens.md` patterns. RLS on every new table. JWT verification in edge functions. Server-side validation. No hardcoded secrets.

## Antipatterns

- Writing tests that pass immediately (means you're testing nothing)
- Weak assertions: `toBeTruthy()`, `not.toBeNull()`, `expect(result).toBeDefined()` â€” these don't verify behavior
- "While I'm here" additions â€” features, refactors, or cleanup not in the spec
- Claiming âœ… on criteria you didn't manually verify
- Ignoring failing tests from other parts of the codebase ("not my problem")
- Writing implementation first, then tests that confirm what you built (circular validation)

## Output Format

When done, report to the manager:

```
## Implementation Complete

**Spec:** {spec path}
**Files changed:** {count}

### Phase 1 â€” Tests Written
- {test file}: {N} tests, all failing correctly
- {test file}: {N} tests, all failing correctly

### Phase 2 â€” Implementation
- {file} â€” {what changed and why}
- {file} â€” {what changed and why}

### Satisfaction Assessment
- âœ… {criterion} â€” implemented and verified
- âš ï¸ {criterion} â€” implemented but {reason for uncertainty}
- âŒ {criterion} â€” not implemented because {reason}
- ğŸ”’ Security: {summary of security posture}

### Verification
- Tested: {what you manually verified} (âœ…/âŒ)
- Could not verify: {what requires environment/QA}

### Test Suite
âœ… {total} passed, 0 failed

### Decisions Made
- {any judgment calls, spec ambiguities resolved, tests modified}

### QA Checklist Items
- {Manual criteria from spec, formatted for QA handoff}

Ready for verification.
```

Update the spec's `**Status:**` from `Approved` to `Implemented`.
